{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is a Jupyter notebook for testing how well the [NER model](https://huggingface.co/dbmdz/flair-historic-ner-onb) for historic German performs on our Travelogues corpus.\n",
    "## Please note the comments that are given in each cell.\n",
    "\n",
    "\n",
    "At first, please try downloading the flair and other packages that are needed for using this model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "import json\n",
    "import ntpath\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "import geocoder\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from correct_ocr import single_characters, delete_specials, correct_s\n",
    "\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "METADATA_PATH: str = '../data/metadata/'\n",
    "VALUES: List[str] = ['Title', 'Contributor']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell is only needed if problems with spacy.load occur."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the model via spacy.load('de_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"de_core_news_md\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "parsed_sentence = nlp(u'This is my sentence. In a huge corpus.')\n",
    "\n",
    "for sent in parsed_sentence.sents:\n",
    "    for token in sent:\n",
    "        print(token.text, token.idx, token.idx + len(token.text) - 1)\n",
    "    #print(sent.start_char, sent.end_char)\n",
    "#print([(token.text,token.idx) for token in parsed_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This part will predict NE on sentences from the Travelogues texts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-16 09:54:45,106 loading file C:\\Users\\patri\\.flair\\models\\flair-historic-ner-onb\\63111d37e8f19b08b01200ec38cd2b093d72026e56bbe99a7b25b6e3f8b7da8d.d53b1d9a206921442955a318ba5bbef2af5aabb93c4713d1ed3b8fe8c28cda3f\n",
      "2022-12-16 09:54:45,855 SequenceTagger predicts: Dictionary with 16 tags: <unk>, O, S-PER, S-LOC, B-PER, E-PER, S-ORG, B-LOC, E-LOC, I-PER, B-ORG, E-ORG, I-LOC, I-ORG, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Load NER model into flair tagger.\n",
    "tagger: SequenceTagger = SequenceTagger.load(\"dbmdz/flair-historic-ner-onb\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: ../data/18th_century_first_quarter_corr\\Z103519403.txt.\n",
      "2662\n",
      "Now predicting Named Entities in ../data/18th_century_first_quarter_corr\\Z103519403.txt.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-24c28b2251a2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msents\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[0msent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0mtagger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tagged_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mentity\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_spans\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'ner'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    478\u001B[0m                 \u001B[1;31m# get features from forward propagation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 479\u001B[1;33m                 \u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgold_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    480\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m                 \u001B[1;31m# remove previously predicted labels of this type\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, sentences)\u001B[0m\n\u001B[0;32m    280\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    281\u001B[0m             \u001B[0msentences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 282\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    283\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    284\u001B[0m         \u001B[1;31m# make a zero-padded tensor for the whole sentence\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\flair\\embeddings\\token.py\u001B[0m in \u001B[0;36membed\u001B[1;34m(self, sentences, static_embeddings)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0membedding\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m             \u001B[0membedding\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\flair\\embeddings\\base.py\u001B[0m in \u001B[0;36membed\u001B[1;34m(self, data_points)\u001B[0m\n\u001B[0;32m     59\u001B[0m             \u001B[0mdata_points\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mdata_points\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 61\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_everything_embedded\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_points\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatic_embeddings\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     62\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_add_embeddings_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_points\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\python36\\lib\\site-packages\\flair\\embeddings\\token.py\u001B[0m in \u001B[0;36m_everything_embedded\u001B[1;34m(self, data_points)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_everything_embedded\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_points\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mSequence\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mSentence\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0msentence\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_points\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 33\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msentence\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     34\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtoken\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_embeddings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m                     \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "# Downloading language model for the spacy pipeline\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Read in files â€“ can be noisy OCR\n",
    "for files in glob.glob('../data/18th_century_first_quarter_corr/*.txt')[:5]:\n",
    "    print(f\"Working on: {files}.\")\n",
    "    file: str = open(files, 'r').read()[:]\n",
    "\n",
    "    # Corrections as implemented by @Lisa Braune\n",
    "\n",
    "    file = re.sub('aÍ¤', 'Ã¤', file)\n",
    "    file = re.sub('uÍ¤', 'Ã¼', file)\n",
    "    file = re.sub('oÍ¤', 'Ã¶', file)\n",
    "\n",
    "    file = correct_s(file)\n",
    "    file = single_characters(file)\n",
    "    file = delete_specials(file)\n",
    "\n",
    "    sents = nltk.sent_tokenize(file, language='german')\n",
    "    print(len(sents))\n",
    "\n",
    "    # Throw document into spacy pipeline, sentencise file\n",
    "    #doc: Doc = nlp(file)\n",
    "    info: dict = {\"type\": \"FeatureCollection\", \"features\": []}\n",
    "    print(f\"Now predicting Named Entities in {files}.\")\n",
    "    idx = 0\n",
    "    for sent in sents:\n",
    "        sent = Sentence(sent)\n",
    "        tagger.predict(sent)\n",
    "        print(sent.to_tagged_string())\n",
    "        for entity in sent.get_spans('ner'):\n",
    "            if entity.get_label(\"ner\").value == 'LOC':\n",
    "                feature_dict = {\n",
    "                    \"type\": \"Feature\", \"properties\": {},\n",
    "                    \"geometry\": {\n",
    "                        \"type\" : \"Point\", \"coordinates\" : []\n",
    "                    }\n",
    "                }\n",
    "                feature_dict[\"properties\"][\"source_label\"] = entity.text\n",
    "                g = geocoder.geonames(entity.text, key='sarahondraszek', featurClass='A')\n",
    "                g_id = g.geonames_id\n",
    "                g = geocoder.geonames(g_id, key='sarahondraszek', method='details')\n",
    "                feature_dict[\"geometry\"][\"coordinates\"] = [g.lng, g.lat]\n",
    "                feature_dict[\"properties\"][\"sentence_idx\"] = idx\n",
    "                feature_dict[\"properties\"][\"start_position\"] = entity.start_position\n",
    "                feature_dict[\"properties\"][\"end_position\"] = entity.end_position\n",
    "                info[\"features\"].append(feature_dict)\n",
    "                #info[barcode][\"score\"] = entity.get_label(\"ner\").score\n",
    "        idx += 1\n",
    "\n",
    "    json_dump = json.dumps(info, indent=4)\n",
    "    with open('../data/test/text_ner/' + os.path.basename(re.sub('.txt', '.json', f'{files}')), 'w') as f:\n",
    "        f.write(json_dump)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NER on Travelogues titles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def read_jsons(metadata_path: str, single_file: bool = False, doc_barcode: str = 'Z124117102', values=None) -> Dict:\n",
    "    \"\"\"Function that aims to retrieve certain metadata information from the JSON files. Can handle single and multiple file attempts.\n",
    "\n",
    "    :param metadata_path: Path for metadata files.\n",
    "    :param single_file: Boolean, False if all files in metadata directory should be observed, True if only a single file should be parsed.\n",
    "    :param doc_barcode: If a single file is parsed, the barcode must be provided.\n",
    "    :param values: List of tags for the information value someone wants to extract, e.g. the title or author of a file.\n",
    "    :return: String of the metadata information.\n",
    "    \"\"\"\n",
    "\n",
    "    output_dict = {}\n",
    "\n",
    "    if values is None:\n",
    "        values = ['Title', 'Contributor']\n",
    "\n",
    "    if single_file:\n",
    "        indicator = doc_barcode\n",
    "    else:\n",
    "        indicator = '*'\n",
    "\n",
    "    for metadata_file in glob.glob(metadata_path + indicator + '.json')[:]:\n",
    "        output_dict[re.sub(r'\\.json', '', ntpath.basename(metadata_file))] = {}\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            object_dict = json.load(f)\n",
    "\n",
    "            for metadata_dict in object_dict:\n",
    "                try:\n",
    "                    if metadata_dict['label'][0]['@value'] in values:\n",
    "                        output_dict[re.sub(r'\\.json', '', ntpath.basename(metadata_file))][metadata_dict['label'][0]['@value']] = metadata_dict['value']\n",
    "                        # print(f\"{metadata_dict['label'][0]['@value']}: {metadata_dict['value']}\")\n",
    "                except TypeError:\n",
    "                    if metadata_dict['label'] in values:\n",
    "                        output_dict[re.sub(r'\\.json', '', ntpath.basename(metadata_file))] = {metadata_dict['label'][0]['@value']: metadata_dict['value']}\n",
    "                        # print(f\"{metadata_dict['label']}: {metadata_dict['value']}\")\n",
    "\n",
    "    return output_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "travelogues_titles: dict = read_jsons(metadata_path=METADATA_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def ner_tagged_jsons(titles: dict, tag: str, ner_model: tagger) -> None:\n",
    "    \"\"\"\n",
    "\n",
    "    :param titles:\n",
    "    :param tag:\n",
    "    :param ner_model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for barcode, items in titles.items():\n",
    "        print(f\"Predicting Named Entities in {barcode}.\")\n",
    "        info: dict = {\"type\": \"FeatureCollection\", \"features\": []}\n",
    "        title_sentence = (items[tag])\n",
    "        ner_model.predict(title_sentence)\n",
    "        for entity in title_sentence.get_spans('ner'):\n",
    "            if entity.get_label(\"ner\").value == 'LOC':\n",
    "                feature_dict = {\n",
    "                    \"type\": \"Feature\", \"properties\": {},\n",
    "                    \"geometry\": {\n",
    "                        \"type\" : \"Point\", \"coordinates\" : []\n",
    "                    }\n",
    "                }\n",
    "                feature_dict[\"properties\"][\"source_label\"] = entity.text\n",
    "                g = geocoder.geonames(entity.text, key='sarahondraszek', featurClass='A')\n",
    "                g_id = g.geonames_id\n",
    "                g = geocoder.geonames(g_id, key='sarahondraszek', method='details')\n",
    "                feature_dict[\"geometry\"][\"coordinates\"] = [g.lng, g.lat]\n",
    "                feature_dict[\"properties\"][\"start_position\"] = entity.start_position\n",
    "                feature_dict[\"properties\"][\"end_position\"] = entity.end_position\n",
    "                info[\"features\"].append(feature_dict)\n",
    "                #info[barcode][\"score\"] = entity.get_label(\"ner\").score\n",
    "\n",
    "        json_dump = json.dumps(info, indent=4)\n",
    "        with open('../data/titles_ner_tagged_jsons/' + barcode + '.json', 'w') as f:\n",
    "            f.write(json_dump)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner_tagged_jsons(titles=travelogues_titles, tag='Title', ner_model=tagger)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
